---
title: "Time-Series"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Time-Series}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Einleitung
In dieser Vignette werden die verschiedenen Funktionen des Packages *TimeSeries* ausführlich erklärt. Das Package enthält Implementationen einger wichtiger Algorithmen zur Analyse Vorhersage von ARMA-Prozessen aus *Introduction to Time-Series Analysis and Forecasting* (Brockwell et. al, 2002):

* ARMA Generator für AR(p) und MA(q) Prozesse *arma_sim*
* Autokovarianz-Funktion für Zeitreihen *ACF*
* Durbin-Levinson-Algorithmus für Zeitreihen *DLA*
* Innovations Algorithmus *innovation* und Vorhersagen mit diesem *ts_predict*
* Periodogram Algorithmus für Zeitreihen *perio*
* Plot Funktion für Zeitreihen *plot_timeseries* und Periodogramme *plot_periodogram*

Um die Funktionen nutzen zu können muss zuerst das Paket *TimeSeries* geladen werden.
```{r setup}
library(TimeSeries)
library(ggplot2)
library(tibble)
```

## ARMA Generator
Ein ARMA(p,q)-Prozess (Autoregressive Moving Average) ist eine stochastischer Prozess der folgendem Modell genügt: $\{X_t\}$ ist stationär und für alle $t$:
$$
X_t = \sum_{i= 1}^{p} \phi_i X_{t-i} + \sum_{j = 1}^{q} \theta_j Z_{t-j} + Z_t \qquad,\ Z_t \sim \mathcal{N}(0, \sigma^2)
$$ 
und die Polynome  $1 + \phi_1X + ... + \phi_p X^p  \text{ und } 1+ \theta_1X + ... + \theta_q X^q$ teilerfremd sind.

(S. 83)

Hier nun ein paar Beispiele:

1. AR(1,1) Prozess:
```{r, fig.width = 7, fig.asp = .5}
arma <- arma_sim(phi = 0.5, theta=0.4,sd=1,I=100)
plot_timeseries(arma)
```

2. MA(1) Prozess:
```{r, fig.width = 7, fig.asp = .5}
arma <- arma_sim(phi = 0.5,sd=1,I=100)
plot_timeseries(arma)
```

3. ARMA(1,1) Prozess:
```{r, fig.width = 7, fig.asp = .5}
arma <- arma_sim(theta=0.4,sd=1,I=100)
plot_timeseries(arma)
```
######Berscghreibuzng was man sieht???

## Autokovarianz-Funktion
Die Autokovarianz ist ein Begriff aus der Signalverarbeitung und gibt die Korrelation einer Funktion bzw. eines Signals mit sich selbst zu einem früheren Zeitpunkt an. Die Funktion gibt also an, wie viel Ähnlichkeit die um eine gewisse Zeitdifferenz verschobene Folge mit der ursprünglichen Folge besitzt. Die Autokovarianz-Funktion ist nach Brookwell et al. (2002) für eine Zeitdifferenz h wie folgt definiert:

$$\gamma_x(h)=Cov(X_{t+h},X_t) $$
Die Funktion *ACF* gibt für eine Zeitreihe $x_{1},...,x_{n}$ die geschätzte Sichtprobenkovarianz für die Zeitdifferenz lag = $0$ bis lag = $n-1$ an. Zusätzlich zur Input Zeitreihe kann die lag-Variable individuell angegeben werden. Es kann also die Anzahl der zurückgegebenen Autokovarianzen begrenzt werden. Dafür wird folgende Formel genutzt:

$$ \hat{\gamma}(h) := n^{-1} \sum_{t=1}^{n-|h|} (x_{t+|h|}-\bar{x})(x_t-\bar{x})$$
````{r , fig.width = 7, fig.asp = .5}
set.seed(3)
ar <-arma_sim(phi = c(0.2,0.6),I=100,sd=0.01)
#ar <- arima.sim(n = 100,list(ar = c(0.2,0.6)))
ar_acf <- ACF(ar, lag = 20)
ma <- arma_sim(theta = c(0.5,0.9),I=100,sd=0.01)
#ma <- arima.sim( n = 100,list(ma = c(0.5,0.9)))
ma_acf <- ACF(ma, lag = 20)
ar_plot <- ggplot(tibble(ACF=ar_acf, Lag=seq_along(ar_acf)), aes(x=Lag, y=ACF)) +
                geom_bar(stat="identity") + 
                ggtitle("AR(1)")
ma_plot <- ggplot(tibble(ACF=ma_acf, Lag=seq_along(ma_acf)), aes(x=Lag, y=ACF)) +
                geom_bar(stat="identity") + 
                ggtitle("MA(1)")
ar_plot
ma_plot
```

Anhand der beiden Autokovarianz-Funktionen kann man eine erste Aussage über die Prozessart treffen. Der erste Plot zeigt eine langsam abnehmende Autokovarianz-Funktion, was auf einen AR-Prozess hindeutet. Der zweite Plot zeigt eine abprupt abnehemnde Autokovarianz-Funktion, was eher auf einen MA-Prozess schließen lässt.

## Durbin-Levinson Algorithmus
Der Durbin-Levinson Algorithmus kann für Vorhersagen einer Zeitreihe genutzt werden. Der Algorithmus benutzt dabei eine Linearkombination aller vorherigen Zeitreihen-Punkte, um einen neuen Wert vorherzusagen. Dies wird duch die folgende Gleichung nach Brookwell et al. (2002) ausgedrückt:

$$\begin{align} 
P_nX_{n+1}=\phi_{n1}X_n+...+\phi_{nn}X_1. 
\end{align}$$

Die entprechenden $\phi's$ werden durch den folgenden Algorithmus rekursiv bestimmt:

$$ \phi_{nn}=\Big[ \gamma(n)-\sum \limits_{j=1}^{n-1}\phi_{n-1,j}\gamma(n-j)\Big]v_{n-1}^{-1}, $$

$$\begin{bmatrix}\phi_{n1} \\ \vdots \\\phi_{n,n-1}  \end{bmatrix}=
\begin{bmatrix}\phi_{n-1,1} \\ \vdots \\\phi_{n-1,n-1}  \end{bmatrix}-
\phi_{nn}
\begin{bmatrix}\phi_{n-1,n-1} \\ \vdots \\\phi_{n-1,1}  \end{bmatrix},$$
und
$$v_n=v_{n-1}[1-\phi_{nn}^2] $$

Für die Startwerte gilt $\phi_{11}=\gamma(1)/\gamma(0)$ und $v_0=\gamma(0)$.

Die Funktion *DLA* benutzt den Durbin-Levinson Algorithums und gibt die geschätzten $\phi's$ zurück. Die Funktion bestimmt standartmäßig die maximale Anzahl an Rekursionen, also n-1. Dieser Wert kann jedoch individuell mit dem *len*-Parameter gesetzt werden. Die erhaltenen $\phi's$ können nun für eine Vorhersage benutzt werden. \n
Im folgenden Beispiel wird die Funktion auf AR-generierte Daten angewendet:
```{r}
set.seed(2)
ar <- arma_sim(phi = c(0.2,0.6),I=100,sd=0.01)
#ar <- arima.sim(n = 100,list(ar = c(0.2,0.5)))
res <- DLA(ar)
```

```{r}
# len=100 (hier: Default)
res
```

```{r}
# len=5
DLA(ar, len=5)
```
Der Output zeigt für die beiden angegebenen Rekursionen die geschätzten $\phi's$ an. Die ersten beiden Werte entsprechen hier nahezu den angegebenen ar-Werten in *arima.sim*. Für die Vorhersage müssen nun die $\phi_{n1},...,\phi_{nn}$ mit der Zeitreihe (außgenommen dem letzten Wert, da die maximale Rekusionszahl n-1 ist) multipliziert und anschließend summiert werden.
```{r}
res <- sum(ar*res[-100])
res
```

## Innovation Algorithmus
Der Innovation Algorithmus ist eine zweite Möglichkeit, um Werte der Zeitreihe vorherzusagen. Sei $\mathbf{\hat X_n} = (X_1,\  P_1 X_2,\ ...\ ,\ P_{n-1}X_n)$.
Dann gibt es eine Matrix $\mathbf{ \Theta_n}$ sodass $\mathbf{\hat X_n} = \mathbf{ \Theta_n} (\mathbf{X_n - } \mathbf{\hat X_n})$. Es gilt also
$$\begin{equation}
\hat{X}_{n+1}= \begin{cases}0, & \text { if } n=0, \\ \sum_{j=1}^{n} \theta_{n j}\left(X_{n+1-j}-\hat{X}_{n+1-j}\right), & \text { if } n=1,2, \ldots,\end{cases}
\end{equation}$$
(S. 72)

Wir können nun die Koeffizienten $\theta_n, ij$ rekursiv berechnen. $\kappa(i,j)$ ist dabei die Kovarianz.
$$
\begin{aligned}
&v_{0}=\kappa(1,1), \\
&\theta_{n, n-k}=v_{k}^{-1}\left(\kappa(n+1, k+1)-\sum_{j=0}^{k-1} \theta_{k, k-j} \theta_{n, n-j} v_{j}\right), \quad 0 \leq k<n,\\
&\ v_{n}=\kappa(n+1, n+1)-\sum_{j=0}^{n-1} \theta_{n, n-j}^{2} v_{j}
\end{aligned}
$$
(S. 73)
Die Funktion *innovation(X, lag = n)* berechnet auf diese Weise $\mathbf\Theta_n$, welches wir dann für die Vorhersage mit *ts_predict* verwenden können.
```{r}
X = arma_sim(theta = c(0.5,0.9),I=5,sd=0.01)
innovation(X, lag = 4)
```

## Innnovation Prediction
Obige Gleichung können wir nun auch verwenden, um eine $h$-Schritt Prognose des Prozesses zu machen. Sei dazu $m$ die Länge der Zeitreihe. Dann berechnen wir mit $\hat X_{m+h}$ eine h-Schritt Prognose. Die Ausgabe von ts_predict ist eine Liste mit den Elementen *X_hat*, der Vektor $ \mathbf{\hat X_{m+h}}$ der Länge $m+h$ und *theta*, die Innovation Matrix $\mathbf{ \Theta_n}$. Die Vorhersage mit dem Innovation Algorithmus eignet sich besonders gut für $MA(q)$-Prozesse mit kleinem $q$


```{r,1. AR(1,1) Prozess:
```{r, fig.width = 7, fig.asp = .5}
I = 50
h = 10
X = arma_sim(theta = c(0.5,0.4),I = I,sd=0.01)
X_hat = ts_predict(X, h)$X_hat
df = tibble(t = 1:(I+h), X = X[1:(I+h)], X_hat = X_hat)
plt = ggplot(df)
lay1 = geom_line(df, mapping = aes(x = t, y = X_hat), color = "red")
lay2 = geom_line(df, mapping = aes(x = t, y = X))
plt + lay1 + lay2
```




## Periodogramm
Das Periodogramm einer Zeitreihe $\{x_1,...,x_n\}$ ist die Funktion, die auf Stichproben basiert einen Schätzung für die Spektraldichte zurückgibt. Die Definition ist dabei nach Brockwell et al. (2002):
$$
I_n(\lambda)=\frac{1}{n} \Bigg| \sum \limits_{t=1}^{n}x_{t}e^{-it\lambda} \Bigg| ^2
$$
Dabei steht der Koeffizient $x_t$ für eine Kombination von Sinusen und Cosinusen:
$$
x_t=\sum \limits_{k \in F_n}a_k [\cos(\omega_kt) + i \sin(\omega_kt)]
$$
wobei $F_n$ für die Fourierfrequenzen der Zeitreihe im Intervall $(-\pi,\pi]$ steht und für $\omega_k$ gilt:
$$\omega_k=\frac{2\pi k}{n}, \space\space\space\space\space\space\space\space k = -[\frac{n-1}{2},...,\frac{n}{2}]$$
Die Folge ${a_k}$ ist die diskrete Fourier Transformation der Zeitreihe und berechet sich nach: 
$$
a_k=\frac{1}{\sqrt{n}}\sum \limits_{t=1}^{n}x_t e^{-it\omega_k}
$$
Die implementierte Funktion *perio* implementiert nun den Periodogramm-Algorithmus und gibt für die $\omega's$ im oben definierten Intervall den Wert $I_n(\lambda)$ zurück.
Der folgende Code veranschaulicht dies:

```{r, fig.width = 7, fig.asp = .5}
x <- arma_sim(phi = c(0.2,-0.6),I=1000,sd=0.01)
#x <-  arima.sim( n = 1000,list(ar = c(0.2,-0.6)))
periodo <- perio(x)
plot_periodogram(periodo) 
```
